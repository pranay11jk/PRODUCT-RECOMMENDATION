{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs from: C:\\Users\\Khan\\Desktop\\ProductRecommendation\n",
      "movies: (27278, 3), ratings: (20000263, 4), genome_scores: (11709768, 3), genome_tags: (1128, 2)\n",
      "user_tags: (465564, 4)\n",
      "Building text documents for movies...\n",
      "Vectorizing item documents with TF-IDF...\n",
      "Item TF-IDF shape: (27278, 50000)\n",
      "Train ratings: (16001027, 4) Test ratings: (3999236, 4)\n",
      "Global train mean rating: 3.525422836921655\n",
      "Building user profiles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138493/138493 [2:33:10<00:00, 15.07it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing item vectors while building profiles: 0\n",
      "Sample recs for user 1 : [(2161, 0.5737094713387488), (98809, 0.5685739429266572), (65685, 0.5672779388649354), (2093, 0.5577199835554792), (2116, 0.5511835861834288)]\n",
      "Evaluating (may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:20<00:00, 49.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results: {'precision@10': np.float64(0.0348), 'recall@10': np.float64(0.019175037278565527), 'f1@10': np.float64(0.020723739538914675), 'map@10': np.float64(0.016176298028470648), 'n_users_eval': 1000}\n",
      "Saved models to: C:\\Users\\Khan\\Desktop\\ProductRecommendation\\cb_model\n",
      "Finished. If you want, I can: \n",
      " - change evaluation to all users (remove n_users_eval),\n",
      " - switch TF-IDF to sentence-transformer embeddings for better semantic matches,\n",
      " - provide a small Flask/FastAPI server to serve recommendations using the saved .pkl.\n"
     ]
    }
   ],
   "source": [
    "# Content-Based Movie Recommender (ready for your folder)\n",
    "# Set DATA_DIR to your folder and run in a Jupyter cell.\n",
    "# Requirements: pandas, numpy, scikit-learn, scipy, joblib, tqdm\n",
    "# Install if needed:\n",
    "# !pip install pandas numpy scikit-learn scipy joblib tqdm\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "# -------------------------\n",
    "# 1) Settings / Paths\n",
    "# -------------------------\n",
    "DATA_DIR = r\"C:\\Users\\Khan\\Desktop\\ProductRecommendation\"   # <-- your folder path\n",
    "SAVE_DIR = os.path.join(DATA_DIR, \"cb_model\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "MOVIES_CSV = os.path.join(DATA_DIR, \"movie.csv\")         # movieId,title,genres\n",
    "RATINGS_CSV = os.path.join(DATA_DIR, \"rating.csv\")      # userId,movieId,rating,timestamp\n",
    "GENOME_SCORES_CSV = os.path.join(DATA_DIR, \"genome_scores.csv\")  # movieId,tagId,relevance\n",
    "GENOME_TAGS_CSV = os.path.join(DATA_DIR, \"genome_tags.csv\")      # tagId,tag\n",
    "TAGS_CSV = os.path.join(DATA_DIR, \"tag.csv\")            # optional user tags\n",
    "LINKS_CSV = os.path.join(DATA_DIR, \"link.csv\")          # optional\n",
    "\n",
    "# -------------------------\n",
    "# 2) Load data (basic checks)\n",
    "# -------------------------\n",
    "print(\"Loading CSVs from:\", DATA_DIR)\n",
    "movies = pd.read_csv(MOVIES_CSV)\n",
    "ratings = pd.read_csv(RATINGS_CSV)\n",
    "genome_scores = pd.read_csv(GENOME_SCORES_CSV)\n",
    "genome_tags = pd.read_csv(GENOME_TAGS_CSV)\n",
    "user_tags = pd.read_csv(TAGS_CSV) if os.path.exists(TAGS_CSV) else None\n",
    "\n",
    "print(f\"movies: {movies.shape}, ratings: {ratings.shape}, genome_scores: {genome_scores.shape}, genome_tags: {genome_tags.shape}\")\n",
    "if user_tags is not None:\n",
    "    print(\"user_tags:\", user_tags.shape)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Build movie \"doc\" (title + genres + weighted genome tags)\n",
    "# -------------------------\n",
    "tagid2tag = dict(zip(genome_tags['tagId'], genome_tags['tag']))\n",
    "TOP_TAGS = 20  # top N tags per movie to include\n",
    "\n",
    "def build_movie_text(movie_id, row):\n",
    "    pieces = []\n",
    "    title = str(row.title) if pd.notnull(row.title) else \"\"\n",
    "    pieces.append(title)\n",
    "    genres = row.genres if pd.notnull(row.genres) else \"\"\n",
    "    pieces.append(genres.replace(\"|\", \" \"))\n",
    "    gs = genome_scores[genome_scores['movieId'] == int(movie_id)]\n",
    "    if not gs.empty:\n",
    "        gs_sorted = gs.sort_values('relevance', ascending=False).head(TOP_TAGS)\n",
    "        tag_terms = []\n",
    "        for _, g in gs_sorted.iterrows():\n",
    "            tag_text = tagid2tag.get(g['tagId'], \"\")\n",
    "            rep = max(1, int(round(g['relevance'] * 10)))  # coarse weighting by repetition\n",
    "            tag_terms.append((\" \" + tag_text) * rep)\n",
    "        if tag_terms:\n",
    "            pieces.append(\" \".join(tag_terms))\n",
    "    # optional: aggregate user tags for the movie\n",
    "    # if user_tags is not None:\n",
    "    #     ut = user_tags[user_tags['movieId'] == movie_id]\n",
    "    #     if not ut.empty:\n",
    "    #         pieces.append(\" \".join(ut['tag'].astype(str).values))\n",
    "    return \" \".join(pieces)\n",
    "\n",
    "print(\"Building text documents for movies...\")\n",
    "movies = movies.copy()\n",
    "movies['movieId'] = movies['movieId'].astype(int)\n",
    "movies['doc'] = movies.apply(lambda r: build_movie_text(int(r.movieId), r), axis=1)\n",
    "movies['doc'] = movies['doc'].fillna(movies['title'].fillna(\"\") + \" \" + movies['genres'].fillna(\"\"))\n",
    "\n",
    "# -------------------------\n",
    "# 4) TF-IDF on movie docs\n",
    "# -------------------------\n",
    "print(\"Vectorizing item documents with TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(max_features=50000, stop_words='english', ngram_range=(1,2))\n",
    "item_tfidf = tfidf.fit_transform(movies['doc'].astype(str))\n",
    "item_tfidf = normalize(item_tfidf)\n",
    "\n",
    "movieid2idx = {int(mid): idx for idx, mid in enumerate(movies['movieId'].astype(int))}\n",
    "idx2movieid = {v: k for k, v in movieid2idx.items()}\n",
    "print(\"Item TF-IDF shape:\", item_tfidf.shape)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Prepare ratings and per-user train/test holdout\n",
    "# -------------------------\n",
    "ratings['movieId'] = ratings['movieId'].astype(int)\n",
    "ratings['userId'] = ratings['userId'].astype(int)\n",
    "\n",
    "min_ratings = 5\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_ratings].index.tolist()\n",
    "ratings_filtered = ratings[ratings['userId'].isin(valid_users)].copy()\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "rng = np.random.RandomState(42)\n",
    "for user, group in ratings_filtered.groupby('userId'):\n",
    "    g = group.sample(frac=1.0, random_state=rng)\n",
    "    test_size = max(1, int(round(0.2 * len(g))))\n",
    "    test = g.iloc[:test_size]\n",
    "    train = g.iloc[test_size:]\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "\n",
    "train_ratings = pd.concat(train_list).reset_index(drop=True)\n",
    "test_ratings = pd.concat(test_list).reset_index(drop=True)\n",
    "print(\"Train ratings:\", train_ratings.shape, \"Test ratings:\", test_ratings.shape)\n",
    "\n",
    "user_train_items = train_ratings.groupby('userId')['movieId'].apply(set).to_dict()\n",
    "user_test_items = test_ratings.groupby('userId')['movieId'].apply(set).to_dict()\n",
    "\n",
    "# -------------------------\n",
    "# 6) Build user profiles (weighted avg of item vectors)\n",
    "# -------------------------\n",
    "global_mean = train_ratings['rating'].mean()\n",
    "print(\"Global train mean rating:\", global_mean)\n",
    "\n",
    "def get_item_vector_by_movieid(mid):\n",
    "    idx = movieid2idx.get(int(mid), None)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return item_tfidf[idx]\n",
    "\n",
    "unique_users = sorted(user_train_items.keys())\n",
    "user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "\n",
    "user_profiles = []\n",
    "missing_item_count = 0\n",
    "print(\"Building user profiles...\")\n",
    "for user in tqdm(unique_users):\n",
    "    ur = train_ratings[train_ratings['userId'] == user].copy()\n",
    "    ur['weight'] = ur['rating'] - global_mean\n",
    "    if np.allclose(ur['weight'].values, 0):\n",
    "        ur['weight'] = ur['rating']\n",
    "    profile = None\n",
    "    total_weight = 0.0\n",
    "    for _, row in ur.iterrows():\n",
    "        v = get_item_vector_by_movieid(row['movieId'])\n",
    "        if v is None:\n",
    "            missing_item_count += 1\n",
    "            continue\n",
    "        w = row['weight']\n",
    "        if profile is None:\n",
    "            profile = v.multiply(w)\n",
    "        else:\n",
    "            profile = profile + v.multiply(w)\n",
    "        total_weight += abs(w)\n",
    "    if profile is None:\n",
    "        profile = csr_matrix((1, item_tfidf.shape[1]))\n",
    "    else:\n",
    "        if total_weight > 0:\n",
    "            profile = profile.multiply(1.0 / total_weight)\n",
    "    user_profiles.append(profile)\n",
    "\n",
    "print(\"Missing item vectors while building profiles:\", missing_item_count)\n",
    "user_profiles_matrix = vstack(user_profiles)\n",
    "user_profiles_matrix = normalize(user_profiles_matrix)\n",
    "\n",
    "# -------------------------\n",
    "# 7) Recommendation function\n",
    "# -------------------------\n",
    "def recommend_for_user(user_id, top_k=10, exclude_train=True):\n",
    "    uidx = user2idx.get(user_id, None)\n",
    "    if uidx is None:\n",
    "        return []\n",
    "    uvec = user_profiles_matrix[uidx]\n",
    "    sims = uvec.dot(item_tfidf.T).toarray().ravel()\n",
    "    train_items = user_train_items.get(user_id, set()) if exclude_train else set()\n",
    "    top_indices = np.argsort(-sims)\n",
    "    recs = []\n",
    "    for idx in top_indices:\n",
    "        mid = idx2movieid[idx]\n",
    "        if mid in train_items:\n",
    "            continue\n",
    "        recs.append((mid, float(sims[idx])))\n",
    "        if len(recs) >= top_k:\n",
    "            break\n",
    "    return recs\n",
    "\n",
    "# test recommendation for one user (if exists)\n",
    "if unique_users:\n",
    "    sample_user = unique_users[0]\n",
    "    print(\"Sample recs for user\", sample_user, \":\", recommend_for_user(sample_user, top_k=5))\n",
    "\n",
    "# -------------------------\n",
    "# 8) Evaluation metrics\n",
    "# -------------------------\n",
    "def precision_at_k(recommended, ground_truth, k):\n",
    "    if len(recommended) == 0: return 0.0\n",
    "    recommended_k = [r[0] for r in recommended[:k]]\n",
    "    hit_count = len(set(recommended_k) & set(ground_truth))\n",
    "    return hit_count / float(k)\n",
    "\n",
    "def recall_at_k(recommended, ground_truth, k):\n",
    "    if len(ground_truth) == 0: return 0.0\n",
    "    recommended_k = [r[0] for r in recommended[:k]]\n",
    "    hit_count = len(set(recommended_k) & set(ground_truth))\n",
    "    return hit_count / float(len(ground_truth))\n",
    "\n",
    "def f1_at_k(p, r):\n",
    "    if p + r == 0: return 0.0\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "def average_precision_at_k(recommended, ground_truth, k):\n",
    "    recommended_k = [r[0] for r in recommended[:k]]\n",
    "    score = 0.0\n",
    "    hits = 0.0\n",
    "    for i, r in enumerate(recommended_k):\n",
    "        if r in ground_truth:\n",
    "            hits += 1.0\n",
    "            score += hits / (i+1.0)\n",
    "    if hits == 0.0: return 0.0\n",
    "    return score / min(len(ground_truth), k)\n",
    "\n",
    "def evaluate_all_users(k=10, users_subset=None, n_users_eval=None):\n",
    "    users = users_subset if users_subset is not None else unique_users\n",
    "    if n_users_eval:\n",
    "        users = users[:n_users_eval]\n",
    "    precs = []; recs = []; f1s = []; maps = []\n",
    "    for user in tqdm(users):\n",
    "        gt = user_test_items.get(user, set())\n",
    "        if not gt:\n",
    "            continue\n",
    "        recs_for_user = recommend_for_user(user, top_k=k, exclude_train=True)\n",
    "        p = precision_at_k(recs_for_user, gt, k)\n",
    "        r = recall_at_k(recs_for_user, gt, k)\n",
    "        f = f1_at_k(p, r)\n",
    "        ap = average_precision_at_k(recs_for_user, gt, k)\n",
    "        precs.append(p); recs.append(r); f1s.append(f); maps.append(ap)\n",
    "    return {\n",
    "        'precision@{}'.format(k): np.mean(precs) if precs else 0.0,\n",
    "        'recall@{}'.format(k): np.mean(recs) if recs else 0.0,\n",
    "        'f1@{}'.format(k): np.mean(f1s) if f1s else 0.0,\n",
    "        'map@{}'.format(k): np.mean(maps) if maps else 0.0,\n",
    "        'n_users_eval': len(precs)\n",
    "    }\n",
    "\n",
    "K = 10\n",
    "print(\"Evaluating (may take a while)...\")\n",
    "eval_results = evaluate_all_users(k=K, n_users_eval=1000)  # evaluate on up to 1000 users (faster); remove n_users_eval for all\n",
    "print(\"Eval results:\", eval_results)\n",
    "\n",
    "# -------------------------\n",
    "# 9) Save artifacts\n",
    "# -------------------------\n",
    "model_artifacts = {\n",
    "    'tfidf_vectorizer': tfidf,\n",
    "    'item_tfidf': item_tfidf,\n",
    "    'movieid2idx': movieid2idx,\n",
    "    'idx2movieid': idx2movieid,\n",
    "    'movies_df': movies[['movieId', 'title', 'genres', 'doc']].copy(),\n",
    "}\n",
    "joblib.dump(model_artifacts, os.path.join(SAVE_DIR, \"cb_model_artifacts.pkl\"))\n",
    "joblib.dump({\n",
    "    'user_profiles_matrix': user_profiles_matrix,\n",
    "    'user2idx': user2idx,\n",
    "    'idx2user': idx2user,\n",
    "    'user_train_items': user_train_items\n",
    "}, os.path.join(SAVE_DIR, \"cb_user_profiles.pkl\"))\n",
    "print(\"Saved models to:\", SAVE_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# 10) Quick helper: load model & recommend for a new user\n",
    "# -------------------------\n",
    "def load_model_and_recommend(model_path, user_rated_items=None, top_k=10):\n",
    "    artifacts = joblib.load(model_path)\n",
    "    tfidf = artifacts['tfidf_vectorizer']\n",
    "    item_tfidf = artifacts['item_tfidf']\n",
    "    movieid2idx = artifacts['movieid2idx']\n",
    "    idx2movieid = artifacts['idx2movieid']\n",
    "    movies_df = artifacts['movies_df']\n",
    "    if user_rated_items is not None:\n",
    "        profile = None\n",
    "        total_w = 0.0\n",
    "        for mid, rating in user_rated_items:\n",
    "            idx = movieid2idx.get(int(mid), None)\n",
    "            if idx is None: continue\n",
    "            v = item_tfidf[idx]\n",
    "            w = float(rating)\n",
    "            if profile is None:\n",
    "                profile = v.multiply(w)\n",
    "            else:\n",
    "                profile = profile + v.multiply(w)\n",
    "            total_w += abs(w)\n",
    "        if profile is None:\n",
    "            return []\n",
    "        if total_w > 0:\n",
    "            profile = profile.multiply(1.0 / total_w)\n",
    "        profile = normalize(profile)\n",
    "        sims = profile.dot(item_tfidf.T).toarray().ravel()\n",
    "        top_idx = np.argsort(-sims)[:top_k]\n",
    "        return [(idx2movieid[i], float(sims[i]), movies_df.loc[movies_df['movieId']==idx2movieid[i],'title'].values[0]) for i in top_idx]\n",
    "    else:\n",
    "        top_idx = np.argsort(-np.array(item_tfidf.sum(axis=1)).ravel())[:top_k]\n",
    "        return [(idx2movieid[i], None, movies_df.loc[movies_df['movieId']==idx2movieid[i],'title'].values[0]) for i in top_idx]\n",
    "\n",
    "# Example usage after saving:\n",
    "# model_path = os.path.join(SAVE_DIR, \"cb_model_artifacts.pkl\")\n",
    "# print(load_model_and_recommend(model_path, user_rated_items=[(1,5.0),(50,4.0)], top_k=10))\n",
    "\n",
    "print(\"Finished. If you want, I can: \\n - change evaluation to all users (remove n_users_eval),\\n - switch TF-IDF to sentence-transformer embeddings for better semantic matches,\\n - provide a small Flask/FastAPI server to serve recommendations using the saved .pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading saved models...\n",
      "‚úÖ Models loaded successfully!\n",
      "üìä Train: (16001027, 4) Test: (3999236, 4)\n",
      "\n",
      "üîç Evaluating model on test users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138493/138493 [50:09<00:00, 46.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Final Evaluation (no @k notation):\n",
      "Precision: 0.0338\n",
      "Recall: 0.0182\n",
      "F1: 0.0197\n",
      "Users_Evaluated: 138493.0000\n",
      "\n",
      "üé¨ Example: Recommendations for a random test user\n",
      "\n",
      "User 77433 top 10 recommendations:\n",
      "\n",
      "  üé• Being There (1979)                        (Score: 0.3236)\n",
      "  üé• Pan's Labyrinth (Laberinto del fauno, El) (2006)  (Score: 0.3213)\n",
      "  üé• Exterminating Angel, The (√Ångel exterminador, El) (1962)  (Score: 0.3203)\n",
      "  üé• Under the Volcano (1984)                  (Score: 0.3165)\n",
      "  üé• Old Boy (2003)                            (Score: 0.3147)\n",
      "  üé• Black Swan (2010)                         (Score: 0.3095)\n",
      "  üé• Adam's Apples (Adams √¶bler) (2005)        (Score: 0.3094)\n",
      "  üé• Lawn Dogs (1997)                          (Score: 0.3047)\n",
      "  üé• 8 1/2 (8¬Ω) (1963)                         (Score: 0.3019)\n",
      "  üé• Storytelling (2001)                       (Score: 0.2977)\n",
      "\n",
      "‚úÖ Done.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# üìä Content-Based Recommendation Model Tester\n",
    "# ================================\n",
    "# Requirements: pandas, numpy, scikit-learn, joblib, tqdm\n",
    "# Run this after training has produced:\n",
    "#   cb_model_artifacts.pkl\n",
    "#   cb_user_profiles.pkl\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# -------------------------\n",
    "# 1) Paths and Load Models\n",
    "# -------------------------\n",
    "DATA_DIR = r\"C:\\Users\\Khan\\Desktop\\ProductRecommendation\"\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"cb_model\")\n",
    "\n",
    "artifacts_path = os.path.join(MODEL_DIR, \"cb_model_artifacts.pkl\")\n",
    "profiles_path = os.path.join(MODEL_DIR, \"cb_user_profiles.pkl\")\n",
    "\n",
    "print(\"üìÇ Loading saved models...\")\n",
    "artifacts = joblib.load(artifacts_path)\n",
    "profiles = joblib.load(profiles_path)\n",
    "\n",
    "tfidf = artifacts[\"tfidf_vectorizer\"]\n",
    "item_tfidf = artifacts[\"item_tfidf\"]\n",
    "movieid2idx = artifacts[\"movieid2idx\"]\n",
    "idx2movieid = artifacts[\"idx2movieid\"]\n",
    "movies_df = artifacts[\"movies_df\"]\n",
    "\n",
    "user_profiles_matrix = profiles[\"user_profiles_matrix\"]\n",
    "user2idx = profiles[\"user2idx\"]\n",
    "idx2user = profiles[\"idx2user\"]\n",
    "user_train_items = profiles[\"user_train_items\"]\n",
    "\n",
    "print(\"‚úÖ Models loaded successfully!\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Load Ratings Data for Evaluation\n",
    "# -------------------------\n",
    "RATINGS_CSV = os.path.join(DATA_DIR, \"rating.csv\")\n",
    "ratings = pd.read_csv(RATINGS_CSV)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].astype(int)\n",
    "ratings[\"userId\"] = ratings[\"userId\"].astype(int)\n",
    "\n",
    "min_ratings = 5\n",
    "user_counts = ratings[\"userId\"].value_counts()\n",
    "valid_users = user_counts[user_counts >= min_ratings].index.tolist()\n",
    "ratings_filtered = ratings[ratings[\"userId\"].isin(valid_users)].copy()\n",
    "\n",
    "# Split into train/test (same logic as before)\n",
    "rng = np.random.RandomState(42)\n",
    "train_list, test_list = [], []\n",
    "for user, group in ratings_filtered.groupby(\"userId\"):\n",
    "    g = group.sample(frac=1.0, random_state=rng)\n",
    "    test_size = max(1, int(round(0.2 * len(g))))\n",
    "    test = g.iloc[:test_size]\n",
    "    train = g.iloc[test_size:]\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "\n",
    "train_ratings = pd.concat(train_list).reset_index(drop=True)\n",
    "test_ratings = pd.concat(test_list).reset_index(drop=True)\n",
    "print(\"üìä Train:\", train_ratings.shape, \"Test:\", test_ratings.shape)\n",
    "\n",
    "user_test_items = test_ratings.groupby(\"userId\")[\"movieId\"].apply(set).to_dict()\n",
    "\n",
    "# -------------------------\n",
    "# 3) Helper Functions\n",
    "# -------------------------\n",
    "def recommend_for_user(user_id, top_k=10, exclude_train=True):\n",
    "    \"\"\"Generate recommendations for a given user.\"\"\"\n",
    "    uidx = user2idx.get(user_id, None)\n",
    "    if uidx is None:\n",
    "        return []\n",
    "    uvec = user_profiles_matrix[uidx]\n",
    "    sims = uvec.dot(item_tfidf.T).toarray().ravel()\n",
    "    train_items = user_train_items.get(user_id, set()) if exclude_train else set()\n",
    "    top_indices = np.argsort(-sims)\n",
    "    recs = []\n",
    "    for idx in top_indices:\n",
    "        mid = idx2movieid[idx]\n",
    "        if mid in train_items:\n",
    "            continue\n",
    "        recs.append((mid, float(sims[idx])))\n",
    "        if len(recs) >= top_k:\n",
    "            break\n",
    "    return recs\n",
    "\n",
    "\n",
    "def precision(pred, true):\n",
    "    if not pred: return 0.0\n",
    "    hits = len(set(pred) & set(true))\n",
    "    return hits / len(pred)\n",
    "\n",
    "def recall(pred, true):\n",
    "    if not true: return 0.0\n",
    "    hits = len(set(pred) & set(true))\n",
    "    return hits / len(true)\n",
    "\n",
    "def f1(p, r):\n",
    "    return 2 * p * r / (p + r) if (p + r) else 0.0\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) Evaluate Model on Test Users\n",
    "# -------------------------\n",
    "print(\"\\nüîç Evaluating model on test users...\")\n",
    "precision_list, recall_list, f1_list = [], [], []\n",
    "\n",
    "for user in tqdm(user_test_items.keys()):\n",
    "    true_items = user_test_items[user]\n",
    "    recs = recommend_for_user(user, top_k=10, exclude_train=True)\n",
    "    pred_items = [m for m, _ in recs]\n",
    "\n",
    "    p = precision(pred_items, true_items)\n",
    "    r = recall(pred_items, true_items)\n",
    "    f = f1(p, r)\n",
    "\n",
    "    precision_list.append(p)\n",
    "    recall_list.append(r)\n",
    "    f1_list.append(f)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Print Final Evaluation\n",
    "# -------------------------\n",
    "results = {\n",
    "    \"Precision\": np.mean(precision_list),\n",
    "    \"Recall\": np.mean(recall_list),\n",
    "    \"F1\": np.mean(f1_list),\n",
    "    \"Users_Evaluated\": len(precision_list)\n",
    "}\n",
    "\n",
    "print(\"\\nüìà Final Evaluation (no @k notation):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Generate Content-Based Recommendations\n",
    "# -------------------------\n",
    "print(\"\\nüé¨ Example: Recommendations for a random test user\\n\")\n",
    "sample_user = np.random.choice(list(user2idx.keys()))\n",
    "recs = recommend_for_user(sample_user, top_k=10)\n",
    "print(f\"User {sample_user} top 10 recommendations:\\n\")\n",
    "\n",
    "for mid, score in recs:\n",
    "    title = movies_df.loc[movies_df[\"movieId\"] == mid, \"title\"].values[0]\n",
    "    print(f\"  üé• {title:<40}  (Score: {score:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
